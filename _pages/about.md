---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

My name is Gangwei Jiang, and I am currently a PhD student in computer science involved in a collaboration between the [University of Science and Technology of China](https://en.ustc.edu.cn/) ([School of Computer Science and Technology](https://en.cs.ustc.edu.cn/main.htm) and the [State Key Laboratory of Cognitive Intelligence](https://cogskl.iflytek.com/)), under the supervision of Prof. [Defu Lian](https://scholar.google.com.hk/citations?user=QW0ad4sAAAAJ&hl=en), and [City University of Hong Kong](https://www.cityu.edu.hk/) ([Department of Computer Science](https://www.cs.cityu.edu.hk/)), co-supervised by Prof. [Ying Wei](https://wei-ying.net/) and Prof. [Linqi Song](https://sites.google.com/site/aisquaredlab/about-us/linqi). I obtained my Bachelor‚Äôs degree in Computer Science and Technology from the University of Science and Technology of China in 2020.

My research interest includes continual learning and large language model. Welcome to reach out to me via email (gwjiang@mail.ustc.edu.cn) if you‚Äôre interested in discussing any related topics.


<!-- # üî• News
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2023</div><img src='images/EMNLP2023-fig1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<ins>Towards anytime fine‚Äëtuning: Continually pre‚Äëtrained language models with hypernetwork prompt<ins> (**EMNLP 2023**)

**Gangwei Jiang**, Caigao Jiang, Siqiao Xue, James Y. Zhang, Jun Zhou, Defu Lian, Ying Wei

[**Paper**](https://arxiv.org/abs/2310.13024) 
- We investigate the challenges of balancing generalization, adaptation, and forgetting in continual pre-training. We propose a continual pre-training algorithm called HnetPrompt-CPT based on hypernetwork prompts, achieving low forgetting and high generalization guarantees.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MM 2022</div><img src='images/MM-fig1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<ins>Self-supervised text erasing with controllable image synthesis<ins> (**MM 2022**)

**Gangwei Jiang**, Shiyao Wang, Tiezheng Ge, Yuning Jiang, Ying Wei, Defu Lian

[**Paper**](https://arxiv.org/pdf/2204.12743) 
- To address the challenge of expensive labeled data for text erasing tasks, we propose a semi-supervised text deletion training framework called STE, which combines efficient text synthesis with a controllable strategy network. This framework outperforms supervised algorithms across multiple datasets.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SIGIR 2021</div><img src='images/SIGIR21-fig1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

<ins>xLightFM: Extremely memory-efficient factorization machine<ins> (**SIGIR 2021**)

**Gangwei Jiang**, Hao Wang, Jin Chen, Haoyu Wang, Defu Lian, Enhong Chen

[**Paper**](http://staff.ustc.edu.cn/~cheneh/paper_pdf/2021/Gangwei-Jiang-SIGIR.pdf) 
- To address the increasing size of recommendation model parameters, we propose a model compression method called xLightFM based on product quantization and neural architecture search, achieving over 10 times efficient compression.
</div>
</div>


# üéñ Honors and Awards
- *2022.10* **China National Scholarship**, USTC, Graduate Students. 
- *2021.10* Huawei Scholarship. 
- *2020 - 2023* First prize, USTC Graduate Student Academic Scholarship.
- *2016 - 2019* First prize, USTC Excellent Undergraduate Scholarship. 

# üìñ Educations
- *2022.09 - now*, PhD student, Department of Computer Science, City University of Hong Kong.
- *2020.09 - now*, PhD student, School of Computer Science and Technology, University of Science and Technology of China.
- *2016.09 - 2020.07*, Undergraduate, School of Computer Science and Technology, University of Science and Technology of China.


# üíª Internships
<!-- - *2021.07 - 2022.05*, [Alimama Smart Creative and AI Application Team](https://github.com/alimama-creative), Alibaba, Beijing. -->
- *2022.09 - 2023.09*, [Dynamic Learning Team](https://github.com/ant-research/EasyTemporalPointProcess), Ant-group, Hangzhou.
- *2021.07 - 2022.05*, [Alimama Smart Creative and AI Application Team](https://github.com/alimama-creative), Alibaba, Beijing.
- *2019.09 - 2020.07*, [Alimama Smart Creative and AI Application Team](https://github.com/alimama-creative), Alibaba, Beijing.
